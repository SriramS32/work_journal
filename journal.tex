\documentclass[a4paper]{article}

%% Language and font encodings
\usepackage[english]{babel}
\usepackage[utf8x]{inputenc}
\usepackage[T1]{fontenc}

%% Sets page size and margins
\usepackage[a4paper,top=3cm,bottom=2cm,left=3cm,right=3cm,marginparwidth=1.75cm]{geometry}

%% Useful packages
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage[colorinlistoftodos]{todonotes}
\usepackage[colorlinks=true, allcolors=blue]{hyperref}

\title{Work/ Idea Journal}
\author{Sriram Somasundaram}
\date{}

\begin{document}
\maketitle

\section{Introduction}

The following is just a journal of my daily activities and thoughts. Many ideas will be cursory without background knowledge, which I may research thoroughly and pursue more in depth.

\section{Days}

\subsection{July 26, 2017}
\subparagraph{Work}
Spent about twenty minutes today with some friends figuring out the best way to implement a n-dimensional array in C using dynamic allocation and trying to make the memory contiguous and easily accessible. I was also trying to understand how Mel-Frequency Cepstral Coefficients (MFCCs) are generated for audio parametrization in speech processing. A lot of the signal processing to parametrize the audio is motivated by how we hear in the cochlea, but the last ste is a Discrete Cosine Transform. Apparently the DCT is similar to Principal Component Analysis, which both involve something called a Karhunen-Lo√®ve transform. Somehow the eigenvectors of the covariance matrix are involved, and I was trying to make sense of what those meant and how using those as a basis minimized mean squared error. Also, I tried out some of the TFLearn library (higher-level API for Tensorflow) just for classification using DNNs and tried to do sequence generation.
\subparagraph{Ideas}
Maybe make a web app template with database support, auth, which uses a couple of modern tools in a pipeline for startups and new developers to use. LSTM for web syntax then link of text descriptions of websites and try to be able to generate websites (HTML, CSS, JS) given a text description of the desired site. Look at what is being done for generating images from text (\href{https://arxiv.org/pdf/1605.05396.pdf}{Ex 1} and \href{https://arxiv.org/pdf/1511.02793.pdf}{2})


\subsection{July 27, 2017}
\subparagraph{Work}
Checked out the tflearn implementation for LSTM and text generation. Thought about how to use the LSTM as a language model that learns word and character sequence probabilities. Needed to refer to Parallel Phone Recognition and Language modeling. Also, was trying to learn about latent semantic analysis and latent dirichlet allocation.
\subparagraph{Ideas}
Gather the titles and topics of arXiv papers and try to predict the direction research is heading to. May have to build some semantic model along with a sequence one.

\subsection{July 30, 2017}
\subparagraph{Work}
Was my birthday yesterday, so I have taken a brief respite. I recently ran into some git ssh key issue after trying to ssh in a cluster I haven't accessed in years to delete data. A quick debug command is to ssh to the git server using a verbose option to see what key is being used and what user is being recognized. Also, may have to check ssh-agent and listing keys in ssh-add.

\subsection{July 31, 2017}
\subparagraph{Work}
Looked a bit at graph databases and the difference between the git services. Also, wanted to learn a bit more about the network programming and sockets: \href{http://beej.us/guide/bgnet/}{Beej's guide}. Was looking at airmon-ng and how to decrypt packets.
\subparagraph{Ideas}
Need to learn about the exact implementation for style transfer. Based on a naive understanding, I believe that some portion of style either from somewhere deep in a convolutional neural net or somewhere else is extracted from an image and applied to one with content. Could one do something similar to voice with i-vectors or some other stylistic representation of one's voice. May have to do with diarization. Really need to look at the papers and think. 

\subsection{August 2, 2017}
\subparagraph{Work}
Looked at protocol buffer and Cap'n Proto. Last day of company work. Made a documentation "guide for dummies" in LaTeX for the Kaldi toolkit that my team worked with that had conceptual, file specific, and implementation details. Went bouldering with coworkers for the first time, and my fingers got blasted.
\subparagraph{Ideas}
Cooler helmet or head protection that fulfills helmet requirements in cities with many motorcyles/ bikes and high temperatures.

\subsection{August 4, 2017}
\subparagraph{Work}
Looked a bit at support vector machines.
\subparagraph{Ideas}
Gathering data from cell providers is quite creepy. However, the data could be tremendously valuable if you couple location info with user data. There are tons of applications that could range from tracking macro patterns (stuff even as minute as demographic that visit theater on opening night of movie) to targeting ads. Another one I thought of today while waiting at a red light: Use live location data (similar to whatever updates waze or google maps on traffic) and figure out the flow of cars through streets to optimize a network of signal light decisions. I know that traffic systems already have complex rules and algorithms in place that incorporate the weight plates, checking if traffic is backed up in a lane, coordinating subsequent lights down a straight road, letting cars one by one into the freeway during peak hours, etc. Anyways, use the data to make better decisions for traffic lights. Manage the flow on demand, optimizing based on the exact data on the streets coupled with existing methods. It is a vague suggestion but worth more investigation.

\subsection{August 10, 2017}
\subparagraph{Ideas}
At some point, I was given some data in a matlab file for MRI segmentation. I was thinking about how the outputs of a classifier would work given images and sequences of images for point classifications. Would they be locations on a standard image file? I would have to use a location invariant classification structure like CNNs. Or would the classifications be curve fitting and coefficient approximation to the edges of the segments. Anyways, I'll try it and see what happens. 

\subsection{August 17, 2017}
\subparagraph{Work}
Learning a bit about Cuda from the \href{http://www.nvidia.com/docs/IO/116711/sc11-cuda-c-basics.pdf}{intro slides} and \href{https://developer.nvidia.com/cuda-example}{Cuda by example book}

\subsection{August 18, 2017}
\subparagraph{Ideas}
Search but queries return back a media experience. Presents a coherent media experience drafting from articles, videos, and possibly audio on google, youtube, journal, and news (maybe not) conveying info about a topic.

\subsection{August 21, 2017}
\subparagraph{Work}
Segmentation links: \href{https://github.com/vuptran/cardiac-segmentation}{link 1} and \href{https://github.com/naldeborgh7575/brain_segmentation}{link 2}.

\subsection{August 22, 2017}
\subparagraph{Ideas}
Develop some sort of classifier for the semantic content of a news article. Could use random forests on the many characteristics if I wanted more syntactical analysis. WOuld need a more complicated model if I was to give the meaning importance. Maybe it could be supplemented with some other data on usage, views, view length, number of views, trusted views, links, almost similar to page rank.
Recently, I have been thinking about getting information based on search queries back in a reliable and trusted fashion. I defintely think there is a space here and something to be improved. Almost like a filtering. Certain news networks may have viral stories, but the credibility of the story coudl be ranked quite low. It is a vague issue because the truthfulness of an article can be a very subjective thing coupled with the issue that we can never be sure of any of these truths in the world other than logical ground zeroes.

\subsection{August 23, 2017}
\subparagraph{Ideas}
Generating a language would involve generating a set of: Layers of speech, text, phonetics, phonology, orthography, morphology, lexemes, syntax, semantics, pragmatics, and discourse. Maybe set up some architecture where a language is created anew, and it would be interesting exploring a seemingly black box.

\subsection{August 24, 2017}
\subparagraph{Ideas}
I occasionally hear of random nuggets of medical wealth being found in various organisms around us. Maybe some genes in sea urchins that protect against Parkinson's, immortality of the jellyfish, etc. If we truly understood the relationship between DNA, RNA transcription/ translation, and protein structure (secondary and tertiary structure), could we use them like building blocks to build predictable systems. Instead of having to find these things naturally, study them, look at their interactions, and then explain why it works, it would be cool if they became as familiar to us as a programming language.

\subsection{August 29, 2017}
\subparagraph{Work}
Looking at the FastMap algorithm to improve heuristic search.

\subsection{September 2, 2017}
\subparagraph{Work}
Looking into FSTs for NLP.

\subsection{September 3, 2017}
\subparagraph{Work}
Had to implement a lot of regular expression work in finite state machines for natural language processing.

\subsection{September 4, 2017}
\subparagraph{Work}
Had to implement a arabic numeral to french number translation using FSTs. Was a pain :).

\end{document}